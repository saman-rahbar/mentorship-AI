{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQl-nIDDCA53"
   },
   "source": [
    "# Case Study - Image Classification using Deep CNN in Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFEKKMXICF9X"
   },
   "source": [
    "<h1>Context<h1/>\n",
    "\n",
    "- The CIFAR-10 dataset contains 60,000 32x32 color images in 10 different classes.\n",
    "- The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
    "- There are 6,000 images of each class.\n",
    "\n",
    "<h2>Understand the labels:<h2/>\n",
    "\n",
    "- airplane : 0\n",
    "- automobile : 1\n",
    "- bird : 2\n",
    "- cat : 3\n",
    "- deer : 4\n",
    "- dog : 5\n",
    "- frog : 6\n",
    "- horse : 7\n",
    "- ship : 8\n",
    "- truck : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Epsl8aAGNPej"
   },
   "source": [
    "## Add-on:\n",
    "What type of classification is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iNIN3s1kHmTB"
   },
   "source": [
    "<h1>Problem Statement<h1/>\n",
    "\n",
    "- Image Classification using Deep CNN in Keras and also some edge detection operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoVbZ22GICCo"
   },
   "source": [
    "<h1>Import all necessary modules and load the data<h1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6bjW33oQvDs"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras import datasets, models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e21_TQyARLdf"
   },
   "outputs": [],
   "source": [
    "# Set the batch size, number of epochs.\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 40\n",
    "num_predictions = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ag1mlSAiRSsR"
   },
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3pXTYLk2Idol",
    "outputId": "70126aeb-3402-4081-a6fe-5a3403f59250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of dataset.\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D99L93w7w3UK",
    "outputId": "9d8c6271-016a-4e6f-df6c-8af904dfbd69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ...,\n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ...,\n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ...,\n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ...,\n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ...,\n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ...,\n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ...,\n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ...,\n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ...,\n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 35, 178, 235],\n",
       "         [ 40, 176, 239],\n",
       "         [ 42, 176, 241],\n",
       "         ...,\n",
       "         [ 99, 177, 219],\n",
       "         [ 79, 147, 197],\n",
       "         [ 89, 148, 189]],\n",
       "\n",
       "        [[ 57, 182, 234],\n",
       "         [ 44, 184, 250],\n",
       "         [ 50, 183, 240],\n",
       "         ...,\n",
       "         [156, 182, 200],\n",
       "         [141, 177, 206],\n",
       "         [116, 149, 175]],\n",
       "\n",
       "        [[ 98, 197, 237],\n",
       "         [ 64, 189, 252],\n",
       "         [ 69, 192, 245],\n",
       "         ...,\n",
       "         [188, 195, 206],\n",
       "         [119, 135, 147],\n",
       "         [ 61,  79,  90]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 73,  79,  77],\n",
       "         [ 53,  63,  68],\n",
       "         [ 54,  68,  80],\n",
       "         ...,\n",
       "         [ 17,  40,  64],\n",
       "         [ 21,  36,  51],\n",
       "         [ 33,  48,  49]],\n",
       "\n",
       "        [[ 61,  68,  75],\n",
       "         [ 55,  70,  86],\n",
       "         [ 57,  79, 103],\n",
       "         ...,\n",
       "         [ 24,  48,  72],\n",
       "         [ 17,  35,  53],\n",
       "         [  7,  23,  32]],\n",
       "\n",
       "        [[ 44,  56,  73],\n",
       "         [ 46,  66,  88],\n",
       "         [ 49,  77, 105],\n",
       "         ...,\n",
       "         [ 27,  52,  77],\n",
       "         [ 21,  43,  66],\n",
       "         [ 12,  31,  50]]],\n",
       "\n",
       "\n",
       "       [[[189, 211, 240],\n",
       "         [186, 208, 236],\n",
       "         [185, 207, 235],\n",
       "         ...,\n",
       "         [175, 195, 224],\n",
       "         [172, 194, 222],\n",
       "         [169, 194, 220]],\n",
       "\n",
       "        [[194, 210, 239],\n",
       "         [191, 207, 236],\n",
       "         [190, 206, 235],\n",
       "         ...,\n",
       "         [173, 192, 220],\n",
       "         [171, 191, 218],\n",
       "         [167, 190, 216]],\n",
       "\n",
       "        [[208, 219, 244],\n",
       "         [205, 216, 240],\n",
       "         [204, 215, 239],\n",
       "         ...,\n",
       "         [175, 191, 217],\n",
       "         [172, 190, 216],\n",
       "         [169, 191, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[207, 199, 181],\n",
       "         [203, 195, 175],\n",
       "         [203, 196, 173],\n",
       "         ...,\n",
       "         [135, 132, 127],\n",
       "         [162, 158, 150],\n",
       "         [168, 163, 151]],\n",
       "\n",
       "        [[198, 190, 170],\n",
       "         [189, 181, 159],\n",
       "         [180, 172, 147],\n",
       "         ...,\n",
       "         [178, 171, 160],\n",
       "         [175, 169, 156],\n",
       "         [175, 169, 154]],\n",
       "\n",
       "        [[198, 189, 173],\n",
       "         [189, 181, 162],\n",
       "         [178, 170, 149],\n",
       "         ...,\n",
       "         [195, 184, 169],\n",
       "         [196, 189, 171],\n",
       "         [195, 190, 171]]],\n",
       "\n",
       "\n",
       "       [[[229, 229, 239],\n",
       "         [236, 237, 247],\n",
       "         [234, 236, 247],\n",
       "         ...,\n",
       "         [217, 219, 233],\n",
       "         [221, 223, 234],\n",
       "         [222, 223, 233]],\n",
       "\n",
       "        [[222, 221, 229],\n",
       "         [239, 239, 249],\n",
       "         [233, 234, 246],\n",
       "         ...,\n",
       "         [223, 223, 236],\n",
       "         [227, 228, 238],\n",
       "         [210, 211, 220]],\n",
       "\n",
       "        [[213, 206, 211],\n",
       "         [234, 232, 239],\n",
       "         [231, 233, 244],\n",
       "         ...,\n",
       "         [220, 220, 232],\n",
       "         [220, 219, 232],\n",
       "         [202, 203, 215]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[150, 143, 135],\n",
       "         [140, 135, 127],\n",
       "         [132, 127, 120],\n",
       "         ...,\n",
       "         [224, 222, 218],\n",
       "         [230, 228, 225],\n",
       "         [241, 241, 238]],\n",
       "\n",
       "        [[137, 132, 126],\n",
       "         [130, 127, 120],\n",
       "         [125, 121, 115],\n",
       "         ...,\n",
       "         [181, 180, 178],\n",
       "         [202, 201, 198],\n",
       "         [212, 211, 207]],\n",
       "\n",
       "        [[122, 119, 114],\n",
       "         [118, 116, 110],\n",
       "         [120, 116, 111],\n",
       "         ...,\n",
       "         [179, 177, 173],\n",
       "         [164, 164, 162],\n",
       "         [163, 163, 161]]]], dtype=uint8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_Ajitha[1, : ,:, :]   #(num_pic, height, width, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[10].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mnXS2u8UNgv_"
   },
   "source": [
    "## Highlights:\n",
    "- How to select the 10th image?\n",
    "- How to get the red pixels only?\n",
    "- What is the shape of resulting array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j-wVAMfGnFij",
    "outputId": "2ab6a7f6-26ba-4a36-84a3-57a3d134fe6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4EU-Du3CeCr"
   },
   "source": [
    "- The training set contains 50000 images.\n",
    "- The size of each image is 32x32 pixels.\n",
    "- Each image has 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K-5Y28Hf7-tq",
    "outputId": "111517e1-1a51-47c3-acb5-24ffb7c12956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint8)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVWDHNjrCy9P"
   },
   "source": [
    "- The label of image at index = 0 is 6:\"frog\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s12yUeMB662K"
   },
   "outputs": [],
   "source": [
    "label_dict =  {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKIYRyxsxCeK"
   },
   "source": [
    "\n",
    "<h2>Explore the Data<h3/>\n",
    "\n",
    "- Understanding a dataset is part of making predictions on the data. \n",
    "- It answers some of questions like in a given data..\n",
    "  - \"What are the possible labels?\"\n",
    "  - \"What is the range of pixel values for the image data?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "66qUQg6OxcsL",
    "outputId": "ee2dba1e-8896-450d-8955-9ec4f0c87fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label \n",
      " Label Id: 6 \n",
      " Name: frog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH3FJREFUeJztnVuMXNd1pv9Vt67qezf7QrJJiRJ1\nGcmxRMmMIEiZjB3PBIoRRDaQZOwHQw9GGAQxEAPJg+AAYw8wD/ZgbMMPAw/okRJl4PFlfImFQJjE\nEWwIiQNFlCXrHomiKLHJVrPJ7mZ3dVXXdc1DlyZUa/+bJTZZTWn/H0B0ca/a56zaddY5VeevtZa5\nO4QQ6ZHZbgeEENuDgl+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvxCJouAXIlEU/EIkSm4rk83sHgBf\nB5AF8D/d/Uux5+fzee8rFoO2VqtF52UQ/hVi1vi+Cjl+XstHbLlsltrMwjs0i5xDIz42m/w1x353\nmY35SH6x2fY231eb780ykRcQod0Ov7aY79HtRfy3yCIzWybiRzbD3092DABAO/JrWY8dCGxOdHth\nFpdXUa6sd7Wziw5+M8sC+O8A/gOAWQBPmNnD7v4Cm9NXLOLA7R8K2paXF+m++jLhN368wBfnqh39\n1DY5PkBtE6OD1FbI5oPjub4SnYMsX+LFpWVqqzf5axsbHaG2TKsRHK/VanTO+vo6tRVL4ZM1ALTA\nT16Vajk4PjI6TOfA+fbqtTq1ZRF+XwB+shka5O/zwAA/PvJ5vh7ViI8eu0BkwsdI7DU3PRzfX37g\nB3w/m3fb9TPfyR0Ajrr7MXevA/gOgHu3sD0hRA/ZSvDPADhx3v9nO2NCiPcAW/nOH/rc8Y7PqmZ2\nCMAhAOjr69vC7oQQl5KtXPlnAew97/97AJza/CR3P+zuB939YC7Pv5sJIXrLVoL/CQDXm9k1ZlYA\n8EkAD18at4QQl5uL/tjv7k0z+yyAv8WG1Peguz8fm7O+vo7nXwg/ZfnMGTpvnNxgtR38zutEa4ja\nrDRFbWttrjqUW+E78G4FOqeyzu/YVqr8DnyjxaWtMxGNs5gL+9hs8u1lyd1mIP5VrbK+Rm3Ndvh1\n2/oOOicTUQEbEbWilOPHQZncMV9sNemc/n5+t98y/NOrETUIABCRDyvrYYWm2QiPA0A2F35fGutV\n7sMmtqTzu/sjAB7ZyjaEENuDfuEnRKIo+IVIFAW/EImi4BciURT8QiTKlu72v1syAEo5IlNFfvx3\nNZH09k3zBJepyXFqK8WknEjWVrUWToBZb3AZyiPbK5QiCUGRxB5v8/2NjIcTmpoNvr1CnvsRSbZE\ntsDftFo9vFaNJl+P/sj2cgPcx2JkXtPCcmQmkiXYjGTgxTJJBwd4Mll5rUJtjWZY0oslVK6unAuO\nt2Nv2Obtd/1MIcT7CgW/EImi4BciURT8QiSKgl+IROnp3X4zR9HCCRVDQ9yVG2bGguM7SjwTJN/m\npanKizzZptXm58NqJex7huf1YDhSFiwXuUu9fG6Vz4u8a+ND4TvOqys8CaceSdCpkqQTIF6XbpCU\nwmrUeeJJpsVfWD6SYNQipcsAIEduz9dqfE4hz9/QTJsnBNXKS9QGkhQGAH3kMG62uSJxbi2s+LQi\n9Rg3oyu/EImi4BciURT8QiSKgl+IRFHwC5EoCn4hEqWnUl/ODGN94V2WIlLOCEnqmBzmNdNapF0U\ngEifGSCbixSSI3XYau2I1BTR5XKR5JJWjUtinuXn7NOnw12AWg3+qlcrPOmk0uKy6GAp0n2nRtp1\ngb/mjHGZKtsX6ZSzxmXd/nzYx1ykFdZ6pO5itcGlvnakydpymfu4XAkfP2UiLQPAeiN8DNQjtRo3\noyu/EImi4BciURT8QiSKgl+IRFHwC5EoCn4hEmVLUp+ZHQewig31rOnuB6M7yxomR8OSzVCeS2zF\nYtiWyXJppRSpj9doctmrHclUcw9LQPVIvb1WncuAbY9kzEUkNs/xrLPVejhDr9Xi61uJtAZrRmyr\na9z/k4thP/IZvr3hMl/7xpu8nVv1HJcqr5q4Ljg+NbWHzrGhcH08AKgtnaW2cplnR55b5VLfmXNh\nWff4Ce5HKxsO3Vqdy4ObuRQ6/0fcnb8zQogrEn3sFyJRthr8DuDvzOxJMzt0KRwSQvSGrX7sv9vd\nT5nZFICfmNlL7v7Y+U/onBQOAUAx8r1eCNFbtnTld/dTnb+nAfwIwB2B5xx294PufrCQ07cMIa4U\nLjoazWzAzIbeegzgNwE8d6kcE0JcXrbysX8awI867a1yAP63u//f2IR8Lovdk+HCjsMFLlEM9oel\nLYtIZYhkWFkkm65W5bJRhsiAO4Z427CBAZ6NtnKOiyQjwzxjbjVSVPP1k+Ftlmv8K1chkgg20x/J\nSszzzMPjZ8PZhTWPFF2NZPWNDA9R2103c4V5ZS4s63olsq8Jni1aq/D1KJf5tbQvz7e5d2f4tU1N\nTdM58yth6fDsy2/SOZu56OB392MAbr3Y+UKI7UVfwoVIFAW/EImi4BciURT8QiSKgl+IROltAc+s\nYXwonG2Xq4elIQDoy4fd7O8L96UDgFqVy2GNSL+10dFwX0AAcFL0sd7i59BGI1JccpD38Tu1EO7F\nBgCvvs6zvRZWw68tUgsSV0d6Hn783x6gtj27uP/ff/JYcPyfjnIpqtnmmYy5DJfmVpcXqK1SDq/j\n0BCX3tDi2YXFIp9XINmnANBvfF6zFX5zrtq7m84ZWgz3cnzmNb4Wm9GVX4hEUfALkSgKfiESRcEv\nRKIo+IVIlN7e7c/lMDW+I2irLvK74hkLu1kmbY4AoBqpZZazSD27SFsrdqasNvhd6tExnqBTb/E7\n2MdmT1Hb4gr3kdX3y0ZafA0X+famcuG7ygBQXOSKxPXDO4Pjc+Pcj/nl09RWq/A1furll6ktQ9pX\nNQYircZGeEINMjxkRka4+jTUjrQHI3Uevb5C5+wjCXJ9+e6v57ryC5EoCn4hEkXBL0SiKPiFSBQF\nvxCJouAXIlF6LPXlMTYxGbSNDfL2WplMOClieWWJzmmslfn2WrF2XbygnZMEo8FBXqevAW578RiX\nqNZqvPVTsdjHbYWwj6UBLkONZbks+uTReWpr1vnhUxsJS32TY3w9DFx+azS5FFyp81qCa6RWX73J\nX7NFpNtINzfkM5FWb5lI7cJceB2bNS6lOpGJSe5ZEF35hUgUBb8QiaLgFyJRFPxCJIqCX4hEUfAL\nkSgXlPrM7EEAvw3gtLv/SmdsHMB3AewDcBzA77s7193+dWsAke0s0s6I0Repp9aPcNYTAOQi57xM\nJlKPj8iAfSXeruvMmzwrrnKGL9m141wSq3HVC0Ui6d24f4bOyUQ22MzyNV6JSK25bLjO4FCBvy87\nxvZT2/7rr6K21954gtpeevlkcLyQi8hozmXiZpOHTIZkVAJAvsDXsd0OH1ftiK5oFj5OI0rkO+jm\nyv+XAO7ZNHY/gEfd/XoAj3b+L4R4D3HB4Hf3xwAsbhq+F8BDnccPAfj4JfZLCHGZudjv/NPuPgcA\nnb9Tl84lIUQvuOw3/MzskJkdMbMjq5XIl1UhRE+52OCfN7NdAND5S+svufthdz/o7geH+vlNLCFE\nb7nY4H8YwH2dx/cB+PGlcUcI0Su6kfq+DeDDACbMbBbAFwB8CcD3zOwzAN4A8Hvd7Kztjup6uFih\nNXhmFhDOwFpb4wUO6w1+Xmtm+CeQcoVLcyvENrOXL6M3+faunuDCzP7dXBqqrPN5MzfcGhwvOP/K\ntXSOF0ItjYYLrgIAzvJMtb07dwXHl9d4tuK1/+Z6ahse41mJw2M3UdvSQnj9l87xlmf5iByZcZ5R\n2WhHskV5sihajfDxHUkSpK3j3kVS34WD390/RUwffRf7EUJcYegXfkIkioJfiERR8AuRKAp+IRJF\nwS9EovS0gKfD0bKwHOItXlCRyRqlIi/6OTjEpaFTC1xWfG12gdpy+bAfhXneV299nm/v+iku5330\nw1z2evXk5lSLf2VoJlwgdWJHuKAmAJxe4EU6R0cjsleb+18gBStPL4Sz7AAgV1ymtoXlOWo7Ocez\n8PL58HEwOsy1t2qVC2ae49dLi2hz7YgMmLHwPItkmEbaPHaNrvxCJIqCX4hEUfALkSgKfiESRcEv\nRKIo+IVIlJ5KfdlsBqOjg0FbM8elvnI5nJHmDS6fnFvlWVuvv8GlrXKZy0alYvhcOfcazy6cLvKi\njjMzV1Pb6O5rqC2/GkkRI0VN99x6B5/yJpffSk0uVbbAMwXX1sK2Xf1hKRIA6i3+umwgfNwAwJ6B\n3dQ2NBqWOFfPvknnnJ4/S20N4/Lmep0XBUWGa3MDfeEs03o1ImGSgqBGZMOgS10/UwjxvkLBL0Si\nKPiFSBQFvxCJouAXIlF6ere/3WpidTl8JzVX57Xu8qQ1EXgJOeSy3FgpcyVgbIgnsowOhO/KVpf4\n3f6p3bwG3swt/47anputU9vLR7ntrl3jwfHlZT5nen+47h8AZFChtnqNKwGjHr5zv3Ka30kv1Xkt\nwV3j4dcFAMstXlcvf8tYcLwaSRT6x0ceprbZE/w1ZyMtuWKNtFgeUSPWVq4RXiuWBBfcRtfPFEK8\nr1DwC5EoCn4hEkXBL0SiKPiFSBQFvxCJ0k27rgcB/DaA0+7+K52xLwL4AwBv6R6fd/dHutlhlige\nrUgSgxOZJEPaeAFAy7jUt8QVJaysROq31cJy2a4RLg/+6kc+Qm17bryT2n74Fw9S285Ikku2Hq5P\nePLYq3x7195MbcUd11HbgHN5trIY7t1aaoelNwCoV7mseGaV20YneRLUjp37guPV8jCdk+EmtAo8\nmSlWw6/R4FKrNcMJauY8ca3ZDIfupZb6/hLAPYHxr7n7gc6/rgJfCHHlcMHgd/fHAPBysUKI9yRb\n+c7/WTN7xsweNDP+WU4IcUVyscH/DQD7ARwAMAfgK+yJZnbIzI6Y2ZFyhX/vEUL0losKfnefd/eW\nu7cBfBMALRPj7ofd/aC7Hxzs51VthBC95aKC38x2nfffTwB47tK4I4ToFd1Ifd8G8GEAE2Y2C+AL\nAD5sZgcAOIDjAP6wm50ZACNKRItkKQG8bVGkcxK8GtlepATe+A7e5mtnf1havP3gDXTOTXdxOW/p\nNJc3+5o88/DaPXuorU1e3M4pXjuvuc4l00okG7De5PMa1fCh1QKXKV89OUttzz53hNruupP7uGNn\nOKtyZTUsRQIA6fAFAJjYx2Xddqy9Vj0i2xEJ+dwCb19WWw072SbZlCEuGPzu/qnA8ANd70EIcUWi\nX/gJkSgKfiESRcEvRKIo+IVIFAW/EInS0wKe7kCbZDBVa1yiKJAstlyOF0zMZrj8c91O/mvkYomf\nD/ddvTc4fuuv8cy9XTfeQm1P/9NfUNtVe7mPOz/wQWorTO4Pjuf6R+icyjqXHKsrPHNv/tQJalua\nD8t2rQbPzisNhQukAsDEBH+vT5x6itqmd80Ex5uVSBZplbfdsrUlamt5OKMSAJxp3ABKfeHXVtjJ\nX/NKH8l0fRcRrSu/EImi4BciURT8QiSKgl+IRFHwC5EoCn4hEqWnUp+ZIZ8N73IpUqCxtR6WNUr9\nJTonm+HSylQkc+/EHM+k2n97qJQhsOeD4fENuGTXWF2jtpEhLs1N3nCA2tZy4Z52zz/1BJ1Tq3I/\nVlb4epw5+Qa1ZVthqbVY5IfczDVhWQ4AbrmBFxJtZnmmXT47Gh4v8KzP3Dov0ll5/SS1MRkbAJqR\ny2yZ9JXs38Ff1zTpAZnPd38915VfiERR8AuRKAp+IRJFwS9Eoij4hUiU3ib2tNuoVcN3Uvv7uCtW\nDN8NzWd4DTlvcVtpkLfy+p3/+DvUdtdvfTQ4PjwxTefMH3uR2rIR/5dXeQ2/heP/Qm2nVsN3nH/2\n139N5wyWeALJeo0nwOyc5orE8FD4TvVrszwZqB5Zj/Hd+6jthg9+iNrQ6gsOLy7zeoEVoi4BwFKV\n+2jOj+H1Kk9cK5MWW17mqsNNYRED7e67denKL0SqKPiFSBQFvxCJouAXIlEU/EIkioJfiETppl3X\nXgB/BWAngDaAw+7+dTMbB/BdAPuw0bLr992dFzgD4HC0ndTWa/OkCGuGZZKmR1pyRWqmFfuGqe3A\nh7hs1JcPS2IvPM1ryC2depXaajUu5awuLVLbiaMvUFvZw8lO+Rbf12COS5/DRZ5cMjnGpb65+TeD\n481IW7bKKpcVT7zGk4iA56mlXA7XICzm+PHR7JuitrNNfuyUSrwGYf8QT0Ir5cJy5Gplhc5ptsOS\n47tQ+rq68jcB/Km73wTgTgB/bGY3A7gfwKPufj2ARzv/F0K8R7hg8Lv7nLv/ovN4FcCLAGYA3Avg\noc7THgLw8cvlpBDi0vOuvvOb2T4AtwF4HMC0u88BGycIAPyzkhDiiqPr4DezQQA/APA5d+dfRt45\n75CZHTGzI2tVXktfCNFbugp+M8tjI/C/5e4/7AzPm9mujn0XgGDDc3c/7O4H3f3gQKlwKXwWQlwC\nLhj8ZmYAHgDwort/9TzTwwDu6zy+D8CPL717QojLRTdZfXcD+DSAZ83s6c7Y5wF8CcD3zOwzAN4A\n8HsX3pRjQy18J+0m/0qQy4dr7rUiNdPq4NlX0yO8rt7fPvw31DY+HZaUpnaF23gBQL3Cs/Py+bDE\nAwCDA1xSymW4NDdA5MidU+GabwBQXeUKbSnLfTy7cIbaGvXwezNU5JJXvcylvleeOkJtcy+9TG21\nJmmhledr2Iqt7x4ufWKAH8OZPi61FolsNwa+Vjd94JrgeKl4jM7ZzAWD393/AQDLcQznuAohrnj0\nCz8hEkXBL0SiKPiFSBQFvxCJouAXIlF6WsATbmi3w8JBIZJZVsyR4ocZXmjRIy2c2nWeWXbmTDgb\nDQDKC2FbqcF/8NgGf13jY1x+G909SW3NVo3aTp4K++iRfK9Mhh8G9SaXTLPGC38OFMPyLEnQ3Nhe\nzBjJ0mzVuZyaIcfbSoXLm/U+Ig8CGNrN136txFubrba5DLi+Fr4G7xi+ls6ZINJtLt99SOvKL0Si\nKPiFSBQFvxCJouAXIlEU/EIkioJfiETprdQHQ8bCWWLFPp7B5CRDb6AUlpMAYGBogtoqDZ5htWOI\n1xzIET/q5+bpnHaGb6+S59LW9HQ4awsA2nUuG914y57g+M9/+iidU/cKteWNy6nVMp83PBTOSizk\n+CGXtUg/u3X+nr02x2W75eXwe1azNTpn8gZ+TZwZjWQlOn+vl87wtSqshyXTgZlIJmYlnDXZjqil\nm9GVX4hEUfALkSgKfiESRcEvRKIo+IVIlJ7e7c8YUMiFzzeVGk+YyJKWUe1IfblKgydnZPM8SaSv\nwO/m5vNhPwr9vG3VyDBPMHpzgasElZnwXXsAmNp7HbWdPB2uq/eBX72bzikvnKK2Yy/zVlhrZZ7I\nksuG139khNcmNFLfEQDmTnIf33g9ktjTF17/4WmuFE2OR3yMqA62yN/rsSUeajNT48HxPaP8GDj6\nQjiBq1blSWub0ZVfiERR8AuRKAp+IRJFwS9Eoij4hUgUBb8QiXJBqc/M9gL4KwA7sdFr67C7f93M\nvgjgDwAsdJ76eXd/JLqznGF6Mny+aZw9S+dVW2EJaI3nZsAzvJVXLpJcMjzMkykKpBVWdY3X8CvF\naqrVue3Iz39ObdfeyCXC2dmwBJSJ1Dvs7+O1+LIRObVU4tLWWjks9VWrXIJtRlq2DZa4H3fddgO1\nFUmCUTPLaxO2GjwJp3qCS32Z1SK1TfUPUdttN3wgPGd0ms55cu614HizwV/XZrrR+ZsA/tTdf2Fm\nQwCeNLOfdGxfc/f/1vXehBBXDN306psDMNd5vGpmLwKYudyOCSEuL+/qO7+Z7QNwG4DHO0OfNbNn\nzOxBM+Otb4UQVxxdB7+ZDQL4AYDPufsKgG8A2A/gADY+GXyFzDtkZkfM7MhKhX+nE0L0lq6C38zy\n2Aj8b7n7DwHA3efdveXubQDfBHBHaK67H3b3g+5+cLifVzoRQvSWCwa/mRmABwC86O5fPW9813lP\n+wSA5y69e0KIy0U3d/vvBvBpAM+a2dOdsc8D+JSZHQDgAI4D+MMLbahQMFy1N3z1HzEukxw9EZZe\n5hd4dl69xaWhwUH+stcqPEOs1S4Hx7ORc+jiApcwV8tclllvcD+yzm1Dg+FbL/NvLtI5s2tcvmo7\nlwinJ7ksau1wdtnSMq+31zfA37PRES6VFbJ8/Wt1IvnmuLy5VuPbq5cjLcrafN51e3dS2+6d4XU8\nMcsl3bML4ZhoxlqebaKbu/3/ACB0BEQ1fSHElY1+4SdEoij4hUgUBb8QiaLgFyJRFPxCJEpPC3hm\nc4bhMZIZR6QLABibyoYNA7wI45l5XhB0PdLuKlfgxRvZtHaDZxA2WtyPc1Uuew1EstjWK1yaq66H\nC3jWIz62IjZ3svYAyiuRdl3D4UKow8O82Gm1yrd35ixfq8FBnl1omfD1zZpcJi7keBHXPq5Io1Dg\na7Xvun3UVq2EfXnssRfonGdePh3e1nr3WX268guRKAp+IRJFwS9Eoij4hUgUBb8QiaLgFyJReir1\nmRlyxfAui8M81398MHyOylW5jJYv8eymlUjfNLT4+bBUnApPyfN9tWq8n12hn/uRz/H1yGa5xFnz\nsC/1Bpc3PZK5Z1wRg9e55Ngipnwkmw4FLm8uL3Gpr1rn/elGRsPSbY5IgACQiax9BVxKmz+zSm1L\nkQzO1bVwlubf/+wlvi+iiq7XJfUJIS6Agl+IRFHwC5EoCn4hEkXBL0SiKPiFSJSeSn3ttqHMCiBm\nB+m8wYGwbpQvcR1qIJJ+NTLCpbnyCu8lV14JF1QsVyJZfevcNlTgBTCLpC8gADRrXOLM5cLn80Lk\nNJ/v49loZnxif6QQaoaYmi0uRRVKkR6Ko1zeXFzkEtsqkT6Hx/naVyI9A185zguyvvTsCWqbHufZ\notN7yGvL8ON0ghQ0nV/lsuc7Nt/1M4UQ7ysU/EIkioJfiERR8AuRKAp+IRLlgnf7zawI4DEAfZ3n\nf9/dv2Bm1wD4DoBxAL8A8Gl3j7bhrdeB2dfDttoyvzs/NBm+Q1wsRRI6uHiA8XH+sstrvI7c8nLY\ntnSWJ4Is8ZvDyLb5Xfa2cyWj1eIKAtphW+wsbxme2JPN8bWqRpKgnNzUz5M2XgDQrPCWYq1Ifb9W\nJFlouRyex7p4AcBiRPE5fpS/octn16itvsZ3uHMk3Mrrpqtn6Bzm4itvrtA5m+nmyl8D8Bvufis2\n2nHfY2Z3AvgygK+5+/UAlgB8puu9CiG2nQsGv2/wVofKfOefA/gNAN/vjD8E4OOXxUMhxGWhq+/8\nZpbtdOg9DeAnAF4FsOz+/z/czQLgn1GEEFccXQW/u7fc/QCAPQDuAHBT6GmhuWZ2yMyOmNmRc2Ve\n/EEI0Vve1d1+d18G8DMAdwIYNbO37gbtAXCKzDns7gfd/eDIYKTjgRCip1ww+M1s0sxGO49LAP49\ngBcB/BTA73aedh+AH18uJ4UQl55uEnt2AXjIzLLYOFl8z93/xsxeAPAdM/svAJ4C8MCFNuSWQys/\nEbQ1CgfpvFo7nMiSaYZbUwFAcYTLV6OT/BPIWIYnnoxXwokWy4u8vdPyGS7nVdf48reaXD6E83N2\nuxn2cb3Kv3IVCpF6gTnu/+o6Tzypkq94+YgaPJQJJ6sAQDvDJaxGg69j30BYMi3meb3A0QL38VqM\nUtsHb+Vtw2685VZq23fddcHxO+7k8ubsqXJw/B9f5TGxmQsGv7s/A+C2wPgxbHz/F0K8B9Ev/IRI\nFAW/EImi4BciURT8QiSKgl+IRDGPZI9d8p2ZLQB4K69vAkD3usTlQ368Hfnxdt5rflzt7pPdbLCn\nwf+2HZsdcXcu7ssP+SE/Lqsf+tgvRKIo+IVIlO0M/sPbuO/zkR9vR368nfetH9v2nV8Isb3oY78Q\nibItwW9m95jZv5jZUTO7fzt86Phx3MyeNbOnzexID/f7oJmdNrPnzhsbN7OfmNkrnb9j2+THF83s\nZGdNnjazj/XAj71m9lMze9HMnjezP+mM93RNIn70dE3MrGhm/2xmv+z48Z8749eY2eOd9fiumUVS\nP7vA3Xv6D0AWG2XArgVQAPBLADf32o+OL8cBTGzDfn8dwO0Anjtv7L8CuL/z+H4AX94mP74I4M96\nvB67ANzeeTwE4GUAN/d6TSJ+9HRNABiAwc7jPIDHsVFA53sAPtkZ/x8A/mgr+9mOK/8dAI66+zHf\nKPX9HQD3boMf24a7PwZgc53qe7FRCBXoUUFU4kfPcfc5d/9F5/EqNorFzKDHaxLxo6f4Bpe9aO52\nBP8MgPPbmW5n8U8H8Hdm9qSZHdomH95i2t3ngI2DEMDUNvryWTN7pvO14LJ//TgfM9uHjfoRj2Mb\n12STH0CP16QXRXO3I/hDJXa2S3K4291vB/BbAP7YzH59m/y4kvgGgP3Y6NEwB+ArvdqxmQ0C+AGA\nz7l7990nLr8fPV8T30LR3G7ZjuCfBbD3vP/T4p+XG3c/1fl7GsCPsL2ViebNbBcAdP6e3g4n3H2+\nc+C1AXwTPVoTM8tjI+C+5e4/7Az3fE1CfmzXmnT2/a6L5nbLdgT/EwCu79y5LAD4JICHe+2EmQ2Y\n2dBbjwH8JoDn4rMuKw9joxAqsI0FUd8Ktg6fQA/WxMwMGzUgX3T3r55n6umaMD96vSY9K5rbqzuY\nm+5mfgwbd1JfBfDn2+TDtdhQGn4J4Ple+gHg29j4+NjAxiehzwDYAeBRAK90/o5vkx//C8CzAJ7B\nRvDt6oEfv4aNj7DPAHi68+9jvV6TiB89XRMAt2CjKO4z2DjR/Kfzjtl/BnAUwP8B0LeV/egXfkIk\nin7hJ0SiKPiFSBQFvxCJouAXIlEU/EIkioJfiERR8AuRKAp+IRLl/wHCOW2RBgdIrQAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "image = x_train[i]\n",
    "label = y_train[i][0]\n",
    "#print(' Label \\n Label Id: {} \\n Name: {}'.format(label, label_dict[label]))\n",
    "print(f'Label \\n Label Id: {label} \\n Name: {label_dict[label]}')\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGR-V6nLnllx"
   },
   "source": [
    "## Add-on:\n",
    "- Without checking, what is the shape of `image`\n",
    "- How to rewrite the print statement using `f-string`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZKivFaZDB7I"
   },
   "source": [
    "- The above image of a frog.\n",
    "- The Label ID is 6.\n",
    "- As we can see the x-axis and y-axis of image, it shows that there are 32 pixels on each directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "aq0rkOOT8nfI",
    "outputId": "962c0320-06df-455b-8b5f-452d63ce93b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff994714438>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sobel = cv2.Sobel(image, cv2.CV_64F, 1, 1, ksize=5)\n",
    "plt.imshow(sobel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0ghOfUzDRP3"
   },
   "source": [
    "- As the image quality is not good, the edges are not so good. But still we can visualize that there are edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9qH3IOGJ8Dwu",
    "outputId": "45c06747-8767-470d-c70d-1a20e430de69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6], dtype=uint8)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l7kVqDH8RUNr"
   },
   "outputs": [],
   "source": [
    "# Convert labels to one hot vectors.\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "enc = LabelBinarizer()\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FtlzWLBqpRw6",
    "outputId": "219b260f-3797-4c46-84a1-6ad4ec2a9e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "w0vmjXIq79Nz",
    "outputId": "8814d436-3c2e-4e5a-d189-4bc7064e1fdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xlCscLsouHY"
   },
   "source": [
    "## Please Note:\n",
    "There are many tools to one-hot encode and they differ in syntax, but the keras one is probably best implemented.\n",
    "- `keras.utils.to_categorical`\n",
    "- `sklearn.preprocessing.OneHotEncoder`\n",
    "- `pandas get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "UWSN8LRypurr",
    "outputId": "80ef1154-e135-40a6-c1ab-fa5ec788dbc0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  1  0  0  0\n",
       "1  0  1  0  0\n",
       "2  0  0  1  0\n",
       "3  0  0  0  1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#y = [[0], [1], [2], [3]]\n",
    "y = [0, 1, 2, 3]\n",
    "#keras.utils.to_categorical(y)\n",
    "#ohe = OneHotEncoder()\n",
    "#ohe.fit_transform(y).toarray()\n",
    "pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRk1hd2llEXG"
   },
   "source": [
    "<h1>Create the Model:<h1/>\n",
    "\n",
    "- Convolutional input layer, 32 feature maps with a size of 5×5 and a rectifier activation function.\n",
    "- Batch Normalization Layer.\n",
    "- Convolutional layer, 32 feature maps with a size of 5×5 and a rectifier activation function.\n",
    "- Batch Normalization layer.\n",
    "- Max Pool layer with size 2×2.\n",
    "- Dropout layer at 25%.\n",
    "---\n",
    "- Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
    "- Batch Normalization layer.\n",
    "- Dropout layer at 25%.\n",
    "- Convolutional layer, 64 feature maps with a size of 3×3 and a rectifier activation function.\n",
    "- Batch Normalization layer.\n",
    "- Max Pool layer with size 2×2.\n",
    "- Dropout layer at 25%.\n",
    "---\n",
    "- GlobalMaxPooling2D layer.\n",
    "- Fully connected layer with 256 units and a rectifier activation function.\n",
    "- Dropout layer at 50%.\n",
    "- Fully connected output layer with 10 units and a softmax activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "G7Q4W4_-DAkL",
    "outputId": "780af4a6-0b17-46ac-ae95-813b1197a68a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 147,658\n",
      "Trainable params: 147,210\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set the CNN model\n",
    "\n",
    "#batch_size = None\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), padding='same', activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(64, (5, 5), padding='same', activation=\"relu\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Conv2D(64, (3, 3), padding='same', activation=\"relu\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.GlobalMaxPooling2D())\n",
    "model.add(layers.Dense(256, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_-yF0WD8hBB"
   },
   "source": [
    "# Number of parameters in the CNN layers:\n",
    "\n",
    "- ## Number of Parameters of a Conv Layer:\n",
    "In a CNN, each layer has two kinds of parameters : weights and biases. The total number of parameters is just the sum of all weights and biases.\n",
    "\n",
    "  Let’s define,\n",
    "\n",
    "  - W_c = Number of weights of the Conv Layer.\n",
    "  - B_c = Number of biases of the Conv Layer.\n",
    "  - P_c = Number of parameters of the Conv Layer.\n",
    "  - K = Size (width) of kernels used in the Conv Layer.\n",
    "  - N = Number of kernels.\n",
    "  - C = Number of channels of the input image.\n",
    "\n",
    "    W<sub>c</sub> = K<sup>2</sup> x C x N\n",
    "    \n",
    "    B<sub>c</sub> = N\n",
    "\n",
    "    P<sub>c</sub> = W<sub>c</sub> + B<sub>c</sub>\n",
    "\n",
    "\n",
    "  So, we will use this formulae for our parameter calculation of first layer:\n",
    "    \n",
    "    model.add(layers.Conv2D(32, (5, 5), padding='same', activation=\"relu\", input_shape=x_train.shape[1:]))\n",
    "\n",
    "  So the number of parameters is given by:\n",
    "\n",
    "  Number of weights = W<sub>c</sub> = 5<sup>2</sup> x 3 x 32 = 2400\n",
    "\n",
    "  Number of biases = B<sub>c</sub> = 32\n",
    "\n",
    "  Total number of parameters for this layer = P<sub>c</sub> = 2400 + 32 = 2432\n",
    "\n",
    "\n",
    "  We can verify this number by looking at the model summary.\n",
    "\n",
    "    conv2d (Conv2D)              (None, 32, 32, 32)        2432\n",
    "\n",
    "- ## Number of Parameters of a MaxPool Layer:\n",
    "There are no parameters associated with a MaxPool layer. The pool size, stride, and padding are hyperparameters.\n",
    "\n",
    "- ## Number of Parameters of a Fully Connected (FC) Layer:\n",
    "There are two kinds of fully connected layers in a CNN. The first FC layer is connected to the last Conv Layer, while later FC layers are connected to other FC layers. Let’s consider each case separately.\n",
    "\n",
    "  - ### Case 1: Number of Parameters of a Fully Connected (FC) Layer connected to a Conv Layer:\n",
    "  Let’s define,\n",
    "\n",
    "    W_{cf} = Number of weights of a FC Layer which is connected to a Conv Layer.\n",
    "\n",
    "    B_{cf} = Number of biases of a FC Layer which is connected to a Conv Layer.\n",
    "    \n",
    "    O = Size (width) of the output image of the previous Conv Layer.\n",
    "    \n",
    "    N = Number of kernels in the previous Conv Layer.\n",
    "    \n",
    "    F = Number of neurons in the FC Layer.\n",
    "\n",
    "      - W<sub>cf</sub> = O<sup>2</sup> x N x F\n",
    "    \n",
    "      - B<sub>cf</sub> = F\n",
    "\n",
    "      - P<sub>cf</sub> = W<sub>cf</sub> + B<sub>cf</sub>\n",
    "\n",
    "    Example: The first fully connected layer of our model is connected to a Conv Layer. For this layer, O = 1 (the width and height of image after global maxpooling), N = 64 and F = 256. Therefore,\n",
    "\n",
    "      - Number of weights = W<sub>cf</sub> = 1<sup>2</sup> x 64 x 256 = 16384\n",
    "\n",
    "      - Number of biases = B<sub>c</sub> = 256\n",
    "\n",
    "      - Total number of parameters for this layer = P<sub>c</sub> = 16384 + 256 = 16640\n",
    "\n",
    "    We can verify this number by looking at the model summary.\n",
    "      \n",
    "            dense (Dense)                (None, 256)               16640 \n",
    "\n",
    "  - ### Case 2: Number of Parameters of a Fully Connected (FC) Layer connected to a FC Layer\n",
    "  Let’s define,\n",
    "\n",
    "    W<sub>ff</sub> = Number of weights of a FC Layer which is connected to an FC Layer.\n",
    "\n",
    "    B<sub>ff</sub> = Number of biases of a FC Layer which is connected to an FC Layer.\n",
    "    \n",
    "    P<sub>ff</sub> = Number of parameters of a FC Layer which is connected to an FC Layer.\n",
    "    \n",
    "    F = Number of neurons in the FC Layer.\n",
    "    \n",
    "    F<sub>-1</sub> = Number of neurons in the previous FC Layer.\n",
    "\n",
    "      - W<sub>ff</sub> = F<sub>-1</sub> x F\n",
    "\n",
    "      - B<sub>ff</sub> = F\n",
    "\n",
    "      - P<sub>ff</sub> = W<sub>ff</sub> + B<sub>ff</sub>\n",
    "\n",
    "    In the above equation, **F<sub>-1</sub> x F** is the total number of connection weights from neurons of the previous FC Layer the neurons of the current FC Layer. The total number of biases is the same as the number of neurons (F).\n",
    "\n",
    "    Example: The last fully connected layer of AlexNet is connected to an FC Layer. For this layer, **F<sub>-1</sub>** = 256 and **F** = 10. Therefore,\n",
    "\n",
    "      - Number of weights = W<sub>cf</sub> = 256 x 10 = 2560\n",
    "\n",
    "      - Number of biases = B<sub>c</sub> = 10\n",
    "\n",
    "      - Total number of parameters for this layer = P<sub>c</sub> = 2560 + 10 = 2570\n",
    "\n",
    "    We can verify this number by looking at the model summary.\n",
    "      \n",
    "            dense_1 (Dense)              (None, 10)                2570"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDIcfXrV0zRH"
   },
   "source": [
    "## Please Note:\n",
    "> Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers.\n",
    "\n",
    "https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGK2OEtgFbF1"
   },
   "source": [
    "<h3>Conv2D:<h3/>\n",
    "\n",
    "- Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
    "\n",
    "<h3>Activation('relu'):<h3/>\n",
    "\n",
    "- 'relu' stands for Rectified linear unit. It is the most widely used activation function. Chiefly implemented in hidden layers of Neural network.\n",
    "- ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n",
    "\n",
    "<h3>MaxPooling2D:<h3/>\n",
    "\n",
    "- The objective MaxPooling Layer is to down-sample an input representation.\n",
    "- This is done to in part to help over-fitting by providing an abstracted form of the representation. As well, it reduces the computational cost by reducing the number of parameters to learn.\n",
    "\n",
    "<h3>Dropout:<h3/>\n",
    "\n",
    "- Dropout is a technique used to improve over-fit on neural networks.\n",
    "- Basically during training half of neurons on a particular layer will be deactivated. This improve generalization.\n",
    "- Normally some deep learning models use Dropout on the fully connected layers, but is also possible to use dropout after the max-pooling layers, creating some kind of image noise augmentation.\n",
    "\n",
    "<h3>Dense:<h3/>\n",
    "\n",
    "- Dense layer implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "\n",
    "<h3>Softmax:<h3/>\n",
    "\n",
    "- The softmax function is also a type of sigmoid function but is handy when we are trying to handle classification problems.\n",
    "- Usually used when trying to handle multiple classes. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs.\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kjBlo11J3GHg"
   },
   "source": [
    "## Add-on:\n",
    "How to handle multilabel classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9Bltmc3mFhQ"
   },
   "source": [
    "<h4>When training the network, what you want is minimize the cost by applying a algorithm of your choice. It could be SGD, AdamOptimizer, AdagradOptimizer, or something. You have to study how each algorithm works to choose what to use, but AdamOptimizer works find for most cases in general.<h4/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwbnUwjGReRR"
   },
   "outputs": [],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDiEZTmyRfmx"
   },
   "outputs": [],
   "source": [
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIAu9SmeSbT_"
   },
   "source": [
    "## Add-on:\n",
    "Which loss function to use? It depends :)\n",
    "\n",
    "https://stackoverflow.com/questions/42081257/why-binary-crossentropy-and-categorical-crossentropy-give-different-performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "colab_type": "code",
    "id": "0uN0HlTjK5ZL",
    "outputId": "173a3581-3294-4fc0-d77d-bf510742decf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 64)        51264     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 147,658\n",
      "Trainable params: 147,210\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network structure is summarized which confirms our design was implemented correctly.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXHjbtgluIyn"
   },
   "source": [
    "- The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
    "\n",
    "- It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
    "\n",
    "- Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdy_QO9tRifH"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') # Conversion to float type from integer type.\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255.0 # Division by 255\n",
    "x_test /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hbe2G_w9bvNh",
    "outputId": "fa44c389-7577-42bd-91c3-6f9062bdb4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint =  ModelCheckpoint('cifar_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jCN4Vz4NPhuU",
    "outputId": "1f5178a3-4252-4ad0-fedc-5ac2a0904f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 32, 32, 3)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32, 3)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7aG-R9GYunmf"
   },
   "source": [
    "<h2>Fit the model:<h2/>\n",
    "\n",
    "-  We can fit this model with 40 epochs and a batch size of 32.\n",
    "\n",
    "- A small number of epochs was chosen to quickly run the code so we can understand the concepts ahead. Normally the number of epochs would be one or two orders of magnitude larger for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "e0ubfQ0kRl1G",
    "outputId": "2b5a09ba-5c87-4f0c-86a2-ccce42f6e2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 1.8571 - accuracy: 0.3172\n",
      "Epoch 00001: val_loss improved from inf to 1.78368, saving model to cifar_cnn_checkpoint_01_loss1.7837.h5\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 1.8571 - accuracy: 0.3172 - val_loss: 1.7837 - val_accuracy: 0.3775\n",
      "Epoch 2/40\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.5134 - accuracy: 0.4457\n",
      "Epoch 00002: val_loss improved from 1.78368 to 1.40452, saving model to cifar_cnn_checkpoint_02_loss1.4045.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.5129 - accuracy: 0.4459 - val_loss: 1.4045 - val_accuracy: 0.4890\n",
      "Epoch 3/40\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.3543 - accuracy: 0.5142\n",
      "Epoch 00003: val_loss did not improve from 1.40452\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.3540 - accuracy: 0.5143 - val_loss: 1.4316 - val_accuracy: 0.4943\n",
      "Epoch 4/40\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 1.2595 - accuracy: 0.5527\n",
      "Epoch 00004: val_loss improved from 1.40452 to 1.34169, saving model to cifar_cnn_checkpoint_04_loss1.3417.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2597 - accuracy: 0.5529 - val_loss: 1.3417 - val_accuracy: 0.5258\n",
      "Epoch 5/40\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 1.1856 - accuracy: 0.5799\n",
      "Epoch 00005: val_loss improved from 1.34169 to 1.18099, saving model to cifar_cnn_checkpoint_05_loss1.1810.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1857 - accuracy: 0.5799 - val_loss: 1.1810 - val_accuracy: 0.5841\n",
      "Epoch 6/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.1352 - accuracy: 0.6011\n",
      "Epoch 00006: val_loss did not improve from 1.18099\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.1349 - accuracy: 0.6013 - val_loss: 1.4635 - val_accuracy: 0.5419\n",
      "Epoch 7/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.0894 - accuracy: 0.6185\n",
      "Epoch 00007: val_loss did not improve from 1.18099\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.0892 - accuracy: 0.6185 - val_loss: 1.2055 - val_accuracy: 0.5955\n",
      "Epoch 8/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 1.0533 - accuracy: 0.6282\n",
      "Epoch 00008: val_loss improved from 1.18099 to 0.96029, saving model to cifar_cnn_checkpoint_08_loss0.9603.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.0529 - accuracy: 0.6282 - val_loss: 0.9603 - val_accuracy: 0.6701\n",
      "Epoch 9/40\n",
      "1554/1563 [============================>.] - ETA: 0s - loss: 1.0293 - accuracy: 0.6384\n",
      "Epoch 00009: val_loss did not improve from 0.96029\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.0291 - accuracy: 0.6384 - val_loss: 1.7472 - val_accuracy: 0.5461\n",
      "Epoch 10/40\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 1.0003 - accuracy: 0.6504\n",
      "Epoch 00010: val_loss improved from 0.96029 to 0.88986, saving model to cifar_cnn_checkpoint_10_loss0.8899.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 1.0009 - accuracy: 0.6503 - val_loss: 0.8899 - val_accuracy: 0.6984\n",
      "Epoch 11/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.9742 - accuracy: 0.6635\n",
      "Epoch 00011: val_loss did not improve from 0.88986\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9744 - accuracy: 0.6634 - val_loss: 1.1633 - val_accuracy: 0.6559\n",
      "Epoch 12/40\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 0.9541 - accuracy: 0.6683\n",
      "Epoch 00012: val_loss improved from 0.88986 to 0.84735, saving model to cifar_cnn_checkpoint_12_loss0.8474.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9538 - accuracy: 0.6683 - val_loss: 0.8474 - val_accuracy: 0.7164\n",
      "Epoch 13/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.9300 - accuracy: 0.6783\n",
      "Epoch 00013: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9302 - accuracy: 0.6782 - val_loss: 1.1448 - val_accuracy: 0.6317\n",
      "Epoch 14/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.9192 - accuracy: 0.6812\n",
      "Epoch 00014: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9192 - accuracy: 0.6812 - val_loss: 0.8974 - val_accuracy: 0.7160\n",
      "Epoch 15/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.8950 - accuracy: 0.6897\n",
      "Epoch 00015: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8950 - accuracy: 0.6897 - val_loss: 0.9561 - val_accuracy: 0.7176\n",
      "Epoch 16/40\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 0.8901 - accuracy: 0.6935\n",
      "Epoch 00016: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8898 - accuracy: 0.6936 - val_loss: 1.0951 - val_accuracy: 0.6605\n",
      "Epoch 17/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.8700 - accuracy: 0.6978\n",
      "Epoch 00017: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8702 - accuracy: 0.6978 - val_loss: 1.0531 - val_accuracy: 0.6657\n",
      "Epoch 18/40\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.8621 - accuracy: 0.7041\n",
      "Epoch 00018: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8621 - accuracy: 0.7041 - val_loss: 1.0416 - val_accuracy: 0.6730\n",
      "Epoch 19/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.8534 - accuracy: 0.7066\n",
      "Epoch 00019: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8536 - accuracy: 0.7065 - val_loss: 0.8819 - val_accuracy: 0.7118\n",
      "Epoch 20/40\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 0.8385 - accuracy: 0.7137\n",
      "Epoch 00020: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8387 - accuracy: 0.7137 - val_loss: 0.9119 - val_accuracy: 0.7030\n",
      "Epoch 21/40\n",
      "1562/1563 [============================>.] - ETA: 0s - loss: 0.8407 - accuracy: 0.7126\n",
      "Epoch 00021: val_loss did not improve from 0.84735\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8407 - accuracy: 0.7126 - val_loss: 1.1337 - val_accuracy: 0.6841\n",
      "Epoch 22/40\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 0.8315 - accuracy: 0.7168\n",
      "Epoch 00022: val_loss improved from 0.84735 to 0.79705, saving model to cifar_cnn_checkpoint_22_loss0.7970.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8311 - accuracy: 0.7170 - val_loss: 0.7970 - val_accuracy: 0.7392\n",
      "Epoch 23/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.8109 - accuracy: 0.7235\n",
      "Epoch 00023: val_loss did not improve from 0.79705\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8112 - accuracy: 0.7235 - val_loss: 0.8574 - val_accuracy: 0.7376\n",
      "Epoch 24/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.8027 - accuracy: 0.7244\n",
      "Epoch 00024: val_loss did not improve from 0.79705\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8023 - accuracy: 0.7245 - val_loss: 0.8581 - val_accuracy: 0.7357\n",
      "Epoch 25/40\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 0.8022 - accuracy: 0.7268\n",
      "Epoch 00025: val_loss improved from 0.79705 to 0.78923, saving model to cifar_cnn_checkpoint_25_loss0.7892.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8023 - accuracy: 0.7268 - val_loss: 0.7892 - val_accuracy: 0.7456\n",
      "Epoch 26/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.7966 - accuracy: 0.7256\n",
      "Epoch 00026: val_loss did not improve from 0.78923\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7969 - accuracy: 0.7254 - val_loss: 0.8192 - val_accuracy: 0.7296\n",
      "Epoch 27/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.7835 - accuracy: 0.7321\n",
      "Epoch 00027: val_loss did not improve from 0.78923\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7833 - accuracy: 0.7322 - val_loss: 1.4873 - val_accuracy: 0.6396\n",
      "Epoch 28/40\n",
      "1558/1563 [============================>.] - ETA: 0s - loss: 0.7786 - accuracy: 0.7351\n",
      "Epoch 00028: val_loss did not improve from 0.78923\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7789 - accuracy: 0.7350 - val_loss: 0.8142 - val_accuracy: 0.7516\n",
      "Epoch 29/40\n",
      "1561/1563 [============================>.] - ETA: 0s - loss: 0.7822 - accuracy: 0.7331\n",
      "Epoch 00029: val_loss improved from 0.78923 to 0.77714, saving model to cifar_cnn_checkpoint_29_loss0.7771.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7820 - accuracy: 0.7331 - val_loss: 0.7771 - val_accuracy: 0.7532\n",
      "Epoch 30/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.7698 - accuracy: 0.7379\n",
      "Epoch 00030: val_loss did not improve from 0.77714\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7698 - accuracy: 0.7377 - val_loss: 0.9543 - val_accuracy: 0.7281\n",
      "Epoch 31/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.7638 - accuracy: 0.7390\n",
      "Epoch 00031: val_loss did not improve from 0.77714\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7639 - accuracy: 0.7390 - val_loss: 1.7996 - val_accuracy: 0.5956\n",
      "Epoch 32/40\n",
      "1555/1563 [============================>.] - ETA: 0s - loss: 0.7615 - accuracy: 0.7393\n",
      "Epoch 00032: val_loss improved from 0.77714 to 0.67426, saving model to cifar_cnn_checkpoint_32_loss0.6743.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7614 - accuracy: 0.7392 - val_loss: 0.6743 - val_accuracy: 0.7813\n",
      "Epoch 33/40\n",
      "1556/1563 [============================>.] - ETA: 0s - loss: 0.7507 - accuracy: 0.7437\n",
      "Epoch 00033: val_loss did not improve from 0.67426\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7507 - accuracy: 0.7437 - val_loss: 0.9061 - val_accuracy: 0.7355\n",
      "Epoch 34/40\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 0.7431 - accuracy: 0.7486\n",
      "Epoch 00034: val_loss improved from 0.67426 to 0.67239, saving model to cifar_cnn_checkpoint_34_loss0.6724.h5\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7433 - accuracy: 0.7484 - val_loss: 0.6724 - val_accuracy: 0.7832\n",
      "Epoch 35/40\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7445 - accuracy: 0.7444\n",
      "Epoch 00035: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7445 - accuracy: 0.7444 - val_loss: 0.6939 - val_accuracy: 0.7708\n",
      "Epoch 36/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.7408 - accuracy: 0.7470\n",
      "Epoch 00036: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7407 - accuracy: 0.7471 - val_loss: 0.8194 - val_accuracy: 0.7481\n",
      "Epoch 37/40\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 0.7407 - accuracy: 0.7464\n",
      "Epoch 00037: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7407 - accuracy: 0.7464 - val_loss: 0.7266 - val_accuracy: 0.7679\n",
      "Epoch 38/40\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.7293 - accuracy: 0.7515\n",
      "Epoch 00038: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7293 - accuracy: 0.7515 - val_loss: 0.9158 - val_accuracy: 0.7307\n",
      "Epoch 39/40\n",
      "1560/1563 [============================>.] - ETA: 0s - loss: 0.7270 - accuracy: 0.7524\n",
      "Epoch 00039: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7269 - accuracy: 0.7525 - val_loss: 0.7541 - val_accuracy: 0.7623\n",
      "Epoch 40/40\n",
      "1557/1563 [============================>.] - ETA: 0s - loss: 0.7311 - accuracy: 0.7491\n",
      "Epoch 00040: val_loss did not improve from 0.67239\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7309 - accuracy: 0.7492 - val_loss: 0.6853 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=True,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping,model_checkpoint])\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhauG6YUvXqb"
   },
   "source": [
    "<h2>Model Score:<h2/>\n",
    "\n",
    "- Once the model is fit, we evaluate it on the test dataset and print out the classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rLXR1tDpRrO2",
    "outputId": "c6327bf0-ebeb-452e-ca15-fccc8ae1ead1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.7838\n",
      "Test loss: 0.6853188276290894\n",
      "Test accuracy: 0.7838000059127808\n"
     ]
    }
   ],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW5reYl7Ls3R"
   },
   "source": [
    "<h3>The above score shows that the accuracy is good, as we used number of epochs = 30.\n",
    "If we use more epochs and tune the hyper-parameters more then we can get some more accuracy score. As our focus on the case study was to learn about the use of CNNs for image classification, we needed to run the code thoroughly so we set number of epochs to less. <h3/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXRH4ZQULJxx"
   },
   "source": [
    "# Summary:\n",
    "\n",
    "<h3>In this case study we discovered how to create deep CNNs in Keras for image classification.<h3/>\n",
    "\n",
    "After working through this case study we learned:\n",
    "\n",
    "- About the CIFAR-10 dataset and how to load it in Keras and plot examples from the dataset.\n",
    "- How to train and evaluate a Convolutional Neural Network on the problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tH0ttsNkzeY_"
   },
   "source": [
    "## Additional Reading Material Links:\n",
    "\n",
    "- Benefits of using pooling layers https://stats.stackexchange.com/questions/288261/why-is-max-pooling-necessary-in-convolutional-neural-networks\n",
    "- Why padding? https://stats.stackexchange.com/questions/246512/convolutional-layers-to-pad-or-not-to-pad\n",
    "- More CNN's   \n",
    "  - https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac\n",
    "- Neural Network Architectures  https://towardsdatascience.com/neural-network-architectures-156e5bad51ba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_S4sC1DdnhR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Session_Case_Study__ComputerVision_Week_2 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('py36': conda)",
   "language": "python",
   "name": "python36264bitpy36condaf3b7c6b548cc4f13ae4649958b51e8aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
